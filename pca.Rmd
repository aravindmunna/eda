---
title: "PCA"
author: "Kathirmani Sukumar"
date: "March 6, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(Matrix)
stocks = read.csv('stock_prices.csv')
```

```{r}
e = eigen(cor(stocks %>% select(-Date, -Samsung)))
e$values
```


```{r}
View(stocks$Google)
sample_mean = mean(stocks$Google)
x = stocks$Google
x_demean = x - sample_mean
x_std = x_demean / sd(x_demean)
sd(x_std)
plot(x_std)

x_std1 = scale(x)
plot(x_std1)
```

```{r}
stocks_std = scale(stocks %>% select(-Date, -Samsung))
e = eigen(cor(stocks_std))
pcs = stocks_std %*% e$vectors
for (i in seq(1:dim(pcs)[2])){
  print (var(pcs[,i]))
}

View(cor(pcs))
```


```{r}
e$values
plot(cumsum(e$values/sum(e$values) * 100), type='l')
cumsum(e$values/length(e$values) * 100)
```

## Reconstruction
```{r}
stocks_rec = pcs %*% t(e$vectors)
dim(stocks_rec)
View(as.data.frame(stocks_rec))
View(as.data.frame(stocks_std))
```

```{r}
pcs_new = pcs
pcs_new[, 2:10] = 0
stocks_rec1 = pcs_new %*% t(e$vectors)
{{plot(stocks_std[,1], type='l', col='red')
lines(stocks_rec1[,1], col='black')}}
```

```{r}
pcs_new = pcs
pcs_new[, 2:10] = 0
stocks_rec1 = pcs_new %*% t(e$vectors)
{{plot(stocks_std[,10], type='l', col='red')
lines(stocks_rec1[,10], col='black')}}
```

```{r}
xc <- 1 # center x_c or h
yc <- 2 # y_c or k
a <- 5 # major axis length
b <- 2 # minor axis length

phi <- pi/3 # angle of major axis with x axis phi or tau

t <- seq(0, 2*pi, 0.01) 
x <- xc + a*cos(t)*cos(phi) - b*sin(t)*sin(phi)
y <- yc + a*cos(t)*cos(phi) + b*sin(t)*cos(phi)
plot(x,y,pch=19, col='blue')
df = data.frame(x=x, y=y)
View(df)
# Scaling the data
df_scale = scale(df)
sd(df_scale[,2])

# eigen values and vectors
e = eigen(cor(df_scale))
e$values
e$vectors

# PCs
pcs = df_scale %*% e$vectors
plot(df_scale[,1], df_scale[,2])
plot(pcs[,1], pcs[,2])

diff(range(df_scale[,1])) # Variation in x axis
diff(range(pcs[,1])) # Variation in PC1 axis


## Case 1: 2nd PC -> 0
pcs_case1 = pcs
pcs_case1[,2] = 0

df_rec1 = pcs_case1 %*% t(e$vectors)
{{plot(df_scale[,1], df_scale[,2])
lines(df_rec1[,1], df_rec1[,2], col='red')}}


## Case 2: 1st Pc -> 0
pcs_case2 = pcs
pcs_case2[,1] = 0
df_rec2 = pcs_case2 %*% t(e$vectors)
{{plot(df_scale[,1], df_scale[,2])
lines(df_rec1[,1], df_rec1[,2], col='red')
lines(df_rec2[,1], df_rec2[,2], col='green')
lines(error_case1, col='blue')}}



error_case1 = df_scale - df_rec1 
plot(error_case1)

pcs = prcomp(df_scale)
biplot(pcs)
pcs$rotation
```

```{r}
x = seq(1,100)
y = x + runif(100, 1,50)
plot(x, y, type='l')

df_scale = scale(data.frame(x=x, y=y))
pcs = df_scale %*% eigen(cor(df_scale))$vectors

pcs_case1 = pcs
pcs_case1[,2] = 0
df_rec1 = pcs_case1 %*% t(eigen(cor(df_scale))$vectors)
{{plot(df_scale)
  lines(df_rec1, col='red')}}

error = df_scale[,2] - df_rec1[,2]
plot(error, type='l')

plot(df_scale - df_rec1)
```

```{r}
org = 5000 * 72
rec = 5000* 5 + 5 * 5
rec/org * 100
```

### PCA Steps
1. Scale your data, For each column
  - Mean = 0,
  - Standard deviation = 1
2. Compute your correlation or covariance
3. Calculate your eigen values and eigen vectors
4. Matrix multiplation between your scaled data and eigen vector
5. Plot your cumulative variance curve to determine how many PCs to choose to retain 95% of variance in the data
6. Pass the retained PCs to your machine learning algorithm

```{r}
stocks_sub = stocks %>% select(-Date, -Samsung)
dim(stocks_sub) # 10 input variables

pcs = prcomp(stocks_sub, scale.=T)
names(pcs)

View(pcs$x) # Principal Components
dim(pcs$x)

View(pcs$rotation) # Eigen vectors
dim(pcs$rotation)

pcs$sdev # square root of Eigen values
sd(stocks_sub[,1])

pcs$scale # Standard deviation of input data before scaling
var(pcs$x[,1])

pcs$center # Mean of each input column before scaling

plot(pcs) # Variance of individual principal components
screeplot(pcs)
# Cumulative variance curve
plot(cumsum(pcs$sdev / sum(pcs$sdev)*100), type='l')

```




```{r}

```

```{r}
### Manual method
df_scale = scale(stocks_1)
e = eigen(cor(df_scale))
pcs = df_scale %*% e$vectors
cum_vars = cumsum(e$values/sum(e$values)*100)
plot(cum_vars, type='l')

### Reconstruct
pcs_new = pcs
pcs_new[,5:dim(df_scale)[2]] = 0
View(pcs_new)
# df_scale_rec = pcs_new %*% t(e$vectors)
# colnames(df_scale_rec) = colnames(stocks_1)



### Methos using prcomp
pcs_comp = prcomp(stocks_1, scale.=T)
e_values = pcs_comp$sdev ^ 2
cum_vars = cumsum(e_values/sum(e_values)* 100)
pcs_comp_new = pcs_comp
pcs_comp_new$x[,5:dim(df_scale)[2]] = 0
# stocks_1_rec = pcs_comp_new$x %*% t(pcs_comp_new$rotation)

biplot(pcs_comp)
```

```{r}
x = rnorm(10)
# y = x * 50
y = rnorm(10)
z = x  + runif(1,1,10)

df = data.frame(x=x, y=y, z=z)
cor(df)
pcs = prcomp(df)
biplot(pcs)
```



```{r}
eigen(cor(stocks_1))$vectors[1:5,1:4]
```


```{r}
eigen(cor(scale(stocks_1)))$vectors[1:5, 1:4]

```


```{r}
plot(pcs$x[,1], pcs$x[,2])
```

```{r}
x = biplot(pcs)
x
```

```{r}
pcs$x
?
```

```{r}
stocks_1 = stocks %>% select(-Date, -Samsung)
stocks_scale = scale(stocks_1)

e = eigen(cor(stocks_scale))
cumsum(e$values/sum(e$values)*100)

fcs = factanal(stocks_scale, factor=6, scores='regression')
fcs$loadings
View(fcs$scores)


library(psych)
fa.out = fa(stocks_scale, nfactors = 3, fm='pa')
fa.diagram(fa.out)
```
```{r}
library(MASS)
fa = scale(Boston)
```

